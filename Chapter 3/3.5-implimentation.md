# 🌐 3.5 — Implementation

## Overview

This section moves from **theory (what routers/switches do)** to **practice (how they’re built)**.  
It explores how network devices evolved from **general-purpose computers** to **specialized, programmable hardware**, and now to **software-defined, cloud-scale systems**.

### Covered Topics
1. **Software Switches** – CPU-driven packet forwarding.  
2. **Hardware Switches** – ASIC/NPU-accelerated high-speed switching.  
3. **Software Defined Networking (SDN)** – disaggregating control and data planes.

---

## 3.5.1 Software Switch

### 🔹 Concept

A **software switch** uses a **general-purpose CPU** and multiple **Network Interface Cards (NICs)** to perform packet forwarding entirely in software.

**Process Flow:**
1. **NIC 1** receives a packet → **DMA** moves it to **main memory**.  
2. **CPU** inspects headers → determines output port.  
3. **NIC 2** transmits the packet → outgoing link.

This is **store-and-forward switching** — packets are buffered in memory between arrival and departure.

---

### 🔹 Performance Bottlenecks

1. **Memory Bandwidth Limits**  
   - All packets traverse **main memory**.  
   - Example: 1333 MHz × 64-bit bus ≈ 100 Gbps — fine for a few 10G ports but not large-scale.

2. **Per-Packet Overhead**  
   - Small packets increase CPU workload.  
   - Example: 40 Mpps × 64 B × 8 = ~20 Gbps → only ~1 Gbps/port on a 16-port switch.

3. **Scaling Limits**  
   - CPU and memory bottlenecks cap software forwarding throughput.

---

### 🔹 Control vs Data Plane

| Plane | Function |
|-------|-----------|
| **Control Plane** | Runs routing protocols (OSPF, RIP, BGP). |
| **Data Plane** | Performs packet forwarding and table lookups. |

In software switches, both planes share the CPU. Performance improves by:
- Optimizing or offloading the data plane.  
- Defining a clear **control–data interface** (foundation of SDN).

🧠 **Analogy:** Control plane = *brains* 🧩; Data plane = *muscle* ⚡

---

## 3.5.2 Hardware Switch

### 🔹 Motivation

Software forwarding can’t reach high throughput.  
**Hardware switches** use **ASICs** for **line-rate packet processing** (full wire speed).

**Challenge:** ASICs are fast but rigid — years to redesign.  
**Goal:** Combine **ASIC speed** with **software flexibility**.

---

### 🔹 Modern Solution — Bare-Metal Switches

Emergence of **Network Processing Units (NPUs)** → **bare-metal switch designs**:

- Commodity, open hardware (e.g., via **Open Compute Project, OCP**).  
- Programmable software stacks (open-source L2/L3).  
- Separation of **hardware** and **network OS**.

**Users can:**
- Purchase base hardware.  
- Install open or custom network OS.  
- Program L2/L3 behavior or hybrids.

---

### 🔹 Architecture Overview

A **bare-metal switch** includes three key components:

| Component | Role |
|------------|------|
| **Control CPU** | Runs control-plane software (routing, config). |
| **Network Processing Unit (NPU)** | Accelerates data-plane tasks (header parsing, forwarding). |
| **Commodity Modules** | Optical/electrical transceivers (e.g., SFP+). |

- NPUs achieve multi-terabit throughput (e.g., 32×100G ports).  
- Architecture mirrors modern computing: modular, open, scalable.

---

### 🔹 Inside the NPU

| Component | Function |
|------------|-----------|
| **SRAM** | Ultra-fast buffer (~10× faster than DRAM). |
| **TCAM** | Supports wildcard header matches using ternary logic. |
| **Forwarding Pipeline** | Multi-stage ASIC path for matching + actions. |

- Each pipeline stage handles one header layer (MAC → IP → ACL).  
- Multiple packets process concurrently → **nanosecond latency**, **Tbps throughput**.

---

### 🔹 Programming the NPU

- **Fixed Pipeline:** Supports standard protocols (Ethernet, IP).  
- **Programmable Pipeline:** Defined via **P4 language**.

**P4** = Open standard defining *Match–Action tables*:  
> “If header = X → then forward to Y.”

Allows flexible forwarding logic without hardware redesign.

---

### 🔹 The Big Picture

Networking now mirrors computing:
- **Open hardware** (OCP).  
- **Programmable software stacks** (P4, open NOS).  
- **Commodity economics** — anyone can assemble a high-performance switch.

---

## 3.5.3 Software Defined Networking (SDN)

### 🔹 Concept

**SDN = Disaggregated Networking**

- Separates **control plane** (decision logic) from **data plane** (packet forwarding).  
- Standardized by **OpenFlow**, defining the control–data interface.

🎯 **Goal:** Any controller can program any hardware switch → breaking vendor lock-in.

---

### 🔹 Logical Centralization

Traditional: Each switch runs its own routing logic.  
SDN: A **logically centralized controller** manages the whole network.

- Maintains a **global network map** (switches, links, ports).  
- Computes optimal routes.  
- Pushes **forwarding rules** into switches.

> “Logically centralized” ≠ single machine — distributed controllers can provide scale & redundancy.

---

### 🔹 SDN in the Cloud

Cloud hyperscalers (Google, Microsoft, AWS) use SDN to:
- Manage data center fabrics and inter-DC backbones.  
- Dynamically reroute traffic.  
- Optimize for latency, utilization, and redundancy.

📈 Since 2012, **virtual (software) switches** have outnumbered physical switches in large data centers.

---

### 🔹 Network Operating System (NOS)

Acts like **Linux for networks**.

**Responsibilities:**
- Maintains global **Network Map**.  
- Detects link/switch changes.  
- Exposes APIs for **Control Applications** (Apps).

Apps define policies (e.g., routing, load balancing, ACLs),  
while the NOS handles distributed coordination.

> Example: A control app can run Dijkstra’s algorithm on the network map directly — no link-state flooding required.

---

### 🔹 Key Takeaways on SDN

- SDN doesn’t replace routing — it **relocates** control logic.  
- Enables **centralized optimization** and **hardware–software decoupling**.  
- Still evolving — trade-offs between **centralized** and **distributed** models.

**Adoption:**  
- Rapid in hyperscale data centers.  
- Gradual in enterprises & telcos due to tooling and expertise gaps.

---

## 🌟 Key Takeaways (Section 3.5)

| Concept | Summary |
|----------|----------|
| **Software Switch** | CPU + NICs forward packets in software; limited by memory and processing. |
| **Control vs Data Plane** | Control = logic, Data = forwarding; foundation for SDN. |
| **Hardware Switch** | ASIC/NPU-accelerated data plane; high-speed, programmable. |
| **Bare-Metal Switch** | Open, modular hardware + software (OCP model). |
| **P4 Language** | Defines programmable match–action pipelines for NPUs. |
| **SDN (Software Defined Networking)** | Separates control/data planes; centralized control via OpenFlow. |
| **Network OS (NOS)** | Provides APIs + global state for control apps. |
| **OpenFlow Interface** | Original SDN protocol linking controllers and switches. |
| **Industry Shift** | Networking is now open, programmable, and software-driven. |

---

## 📘 Glossary

| Term | Definition |
|------|-------------|
| **Software Switch** | Packet forwarding using CPU + NICs entirely in software. |
| **NIC (Network Interface Card)** | Hardware interface connecting to a network. |
| **DMA (Direct Memory Access)** | Moves packets between NIC and memory without CPU. |
| **Control Plane** | Manages routing and configuration. |
| **Data Plane** | Performs per-packet forwarding. |
| **ASIC (Application-Specific Integrated Circuit)** | Custom hardware optimized for packet processing. |
| **NPU (Network Processing Unit)** | Specialized processor for packet parsing and forwarding. |
| **SRAM / DRAM** | Memory types — SRAM is faster, used for buffers. |
| **TCAM (Ternary Content Addressable Memory)** | High-speed memory supporting wildcard matching. |
| **Forwarding Pipeline** | Multi-stage matching/action process in hardware. |
| **P4 Language** | Open language for defining programmable packet pipelines. |
| **Bare-Metal Switch** | Commodity switch hardware with open software stack. |
| **SDN (Software Defined Networking)** | Separates control from data for flexibility. |
| **OpenFlow** | Protocol linking SDN controllers and switches. |
| **NOS (Network Operating System)** | Manages global network state and provides APIs. |
| **Network Map** | Logical view of network topology. |
| **Control App** | Application implementing policies via the NOS. |
| **Disaggregation** | Separation of hardware and software layers. |
| **Logical Centralization** | Global, unified view of the network across distributed controllers. |

---
